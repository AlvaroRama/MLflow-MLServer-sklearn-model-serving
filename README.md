# MLflow & MLServer Sklearn Model Serving Example

This repository contains a complete example of how to build an MLOps service using **MLflow** and **MLServer** for training, tracking, and deploying a Machine Learning model. The project is orchestrated with **Docker Compose** and uses **Dockerize** to ensure the execution order of the inference services.

---

## Project Features

* **Training and Tracking (MLflow):**
    * A container (`trainer`) is responsible for training a Scikit-learn model.
    * MLflow is used to log and track model metrics, parameters, and artifacts.
* **UI and MLOps Services (MLflow UI):**
    * A container (`mlflow-ui`) exposes the MLflow user interface to visualize experiments.
* **Inference with MLflow Serving:**
    * A container (`mlflow-server`) deploys the model registered in MLflow for inference. This is useful for quick tests and compatibility with the MLflow ecosystem.
* **Inference with MLServer:**
    * A container (`model-server`) deploys the model using MLServer, which is ideal for high-performance production environments and customisation.
    * It includes a **custom runtime (`CustomSKLearnModel`)** for MLServer, which explicitly handles the transformation of input data into a `pandas.DataFrame` with column names. This is crucial for pipelines that use `ColumnTransformer` or other Scikit-learn preprocessors.
* **Orchestration with Docker Compose:**
    * Defines and manages the four services (`trainer`, `mlflow-ui`, `mlflow-server`, `model-server`) in an isolated environment.
* **Execution Control with Dockerize:**
    * Uses `dockerize` to ensure that inference services (`mlflow-server` and `model-server`) do not start until the `trainer` has completed training and model saving, guaranteeing the availability of necessary artifacts.

---

## Project Structure

```
├── Dockerfile                  # Defines the base image for all services
├── docker-compose.yml          # Orchestration of Docker services
├── models/                     # Contains MLServer configuration and custom runtime
│   ├── sklearn-model/
│   │   ├── custom_sklearn_runtime.py # Custom runtime for MLServer
│   │   ├── model-settings.json       # Model configuration for MLServer
│   │   └── model.joblib              # Saved scikit-learn model (generated by trainer)
│   └── init.py
├── mlruns/                     # MLflow tracking directory (mounted as a volume)
│   └── ... (artifacts, metrics, etc., generated by the trainer)
├── requirements.txt            # Python dependencies for all services
├── scripts/
│   └── train.py                # Training script that uses MLflow to log the model
├── .dockerignore               # Files to ignore during Docker image build
├── .gitignore                  # Files to ignore in Git
└── README.md                   # This file
```

---

---

## Requirements

* [Docker Desktop](https://www.docker.com/products/docker-desktop/) (includes Docker Engine and Docker Compose)
* Python 3.7+ (to run client scripts and the `trainer` if running locally)
* `requests` and `pandas` (for the inference client script)

---

## Getting Started

1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/AlvaroRama/MLflow-MLServer-sklearn-model-serving.git](https://github.com/AlvaroRama/MLflow-MLServer-sklearn-model-serving.git)
    cd MLflow-MLServer-sklearn-model-serving
    ```

2.  **Build and Bring Up Containers:**
    This command will build the Docker images (if they don't exist or have changed) and bring up all services defined in `docker-compose.yml` in the background.

    ```bash
    docker compose up --build -d
    ```
    * The `trainer` service will run first, training the model and saving artifacts.
    * `mlflow-server` and `model-server` will wait for the `model.joblib` generated by the `trainer` using `dockerize` before starting.
    * `mlflow-ui` will start immediately.

3.  **Verify Service Status:**
    You can check that all containers are running with:

    ```bash
    docker compose ps
    ```
    You should see all 4 services (`mlflow-server`, `mlflow-ui`, `model-server`, `trainer`) with a `Up` status.

---

## Explore MLflow UI

Once the services are up, you can access the MLflow user interface to view the training experiments:

* Open your web browser and navigate to: [http://localhost:5000](http://localhost:5000)

Here you'll be able to see the details of the `trainer` run, including metrics, parameters, and model artifacts.

---

## Perform Inferences

The project offers two inference endpoints for the trained model: one with MLflow Serving and another with MLServer.

### 1. Inference with MLflow Serving

You can make a POST request to the MLflow Serving API on port `1234`.

```bash
curl -X POST http://localhost:1234/invocations \
    -H "Content-Type: application/json" \
    -d '{
        "dataframe_split": {
            "columns": ["MedInc","HouseAge","AveRooms","AveBedrms","Population","AveOccup","Latitude","Longitude"],
            "data": [[8.3252,41.0,6.9841,1.0238,322.0,2.5556,37.88,-122.23]]
        }
    }'
```
### Expected Output

```json
{"predictions": [4.311449081302498]}
```

### 2. Inference with MLServer

Inference with MLServer is performed on port 8080, using the CustomSKLearnModel runtime to ensure data is passed as a DataFrame.

You can use a Python script (e.g., mlserver_inference_client.py, if you've created one in your repo) to make the call. Alternatively, here's the direct code:

```Python

import requests
import json
import pandas as pd
from sklearn.datasets import fetch_california_housing # Needed for load_sample_data

# Function to load sample data (include if using a dedicated script)
def load_sample_data(num_samples=1):
    housing = fetch_california_housing(as_frame=True)
    return housing.data.head(num_samples).values.tolist()

# URL of the inference endpoint
url = "http://localhost:8080/v2/models/sklearn-model/infer"

# Data for inference (you can use loaded data or define directly)
input_values = [8.3252, 41.0, 6.9841, 1.0238, 322.0, 2.5556, 37.88, -122.23]

# MLServer's V2 API expects a specific format
inference_payload = {
    "inputs": [
        {
            "name": "input_data", # Matches model-settings.json
            "shape": [1, 8],      # 1 row, 8 columns
            "datatype": "FP64",
            "data": [input_values],
            "parameters": {
                "content_type": "pd:dataframe.split" # Important for CustomSKLearnModel
            }
        }
    ]
}

print(f"Sending request to: {url}")
print(f"Request payload:\n{json.dumps(inference_payload, indent=2)}")

try:
    response = requests.post(url, json=inference_payload)
    response.raise_for_status() # Raise an exception for HTTP errors

    print("\nServer response (success):")
    print(json.dumps(response.json(), indent=2))

except requests.exceptions.RequestException as e:
    print(f"\nError making request: {e}")
    if response:
        print("HTTP Status Code:", response.status_code)
        print("Response Body (error):", response.text)
```

Expected Output (similar):

```JSON

{
  "model_name": "sklearn-model",
  "id": "...",
  "parameters": {},
  "outputs": [
    {
      "name": "prediction",
      "shape": [
        -1,
        1
      ],
      "datatype": "FP64",
      "data": [
        4.311449081302498
      ]
    }
  ]
}
```

Both predictions should be identical, confirming that both inference services are functioning correctly.

Clean Up Your Environment
To stop and remove all containers and networks created by Docker Compose:

```Bash

docker compose stop
docker compose down
```

This will also remove volumes, which cleans up the mlruns and models data generated.

Contributions
Contributions are welcome. If you have improvements or find issues, feel free to open an issue or submit a pull request.